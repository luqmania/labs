{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.6"},"colab":{"name":"Lab Extra 02 - Sentiment Analysis Id.ipynb","provenance":[{"file_id":"https://github.com/project303/Labs247/blob/master/Text%20Mining/Lab%20Extra%2002%20-%20Sentiment%20Analysis%20Id.ipynb","timestamp":1567498548731}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"jfUnoZ8_Kw_o","colab_type":"text"},"source":["# SENTIMENT ANALYSIS\n","\n"]},{"cell_type":"code","metadata":{"id":"hhGLLyYIKw_2","colab_type":"code","colab":{}},"source":["#Release: 1.1907.1601"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HYTx-IJYKxAR","colab_type":"text"},"source":["# Library\n","\n","For this lab, we will need ``wordcloud`` library.\n","Use pip to install the library from Anaconda prompt : ``pip install wordcloud``."]},{"cell_type":"code","metadata":{"id":"-bvihJgEKxAX","colab_type":"code","colab":{}},"source":["import numpy as np \n","import pandas as pd\n","from sklearn.model_selection import train_test_split \n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.classify import SklearnClassifier\n","\n","from wordcloud import WordCloud,STOPWORDS\n","import matplotlib.pyplot as plt\n","\n","from subprocess import check_output\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4hPLlLJqKxAu","colab_type":"text"},"source":["# Read Data"]},{"cell_type":"code","metadata":{"id":"zlYqy7auLBS1","colab_type":"code","colab":{}},"source":["!git clone https://github.com/project303/dataset.git"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P8Uz_TQ0KxA0","colab_type":"code","colab":{}},"source":["data = pd.read_csv('dataset/Twitter.csv', sep='|')\n","# Choose the column we will be using\n","data = data[['text','sentiment']]\n","len(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yu2KSshuKxBJ","colab_type":"code","colab":{}},"source":["data[:10]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Grlf4XI4KxBb","colab_type":"code","colab":{}},"source":["# Bagi dataset menjadi data training dan testing\n","train, test = train_test_split(data,test_size = 0.2)\n","# Hapus sentiment yang netral\n","train = train[train.sentiment != \"Neutral\"]\n","\n","train_pos = train[ train['sentiment'] == 'Positif']\n","train_pos = train_pos['text']\n","train_neg = train[ train['sentiment'] == 'Negatif']\n","train_neg = train_neg['text']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z36FkkNiKxBp","colab_type":"code","colab":{}},"source":["train_pos"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bdkZE1aKxB3","colab_type":"code","colab":{}},"source":["train_neg"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6_4CCysbKxCi","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"RJe94w56KxCn","colab_type":"code","colab":{}},"source":["#augment the stopwords with nonstandard twitter words\n","stopwords_set = set(stopwords.words(\"indonesian\"))\n","stopwords_aug = {\"ya\",\"yak\",\"iya\",\"yg\",\"ga\",\"gak\",\"gk\",\"udh\",\"sdh\",\"udah\",\"dah\",\"nih\",\"ini\",\"deh\",\"sih\",\"dong\",\"donk\",\n","                 \"sm\",\"knp\",\"utk\",\"yaa\",\"tdk\",\"gini\",\"gitu\",\"bgt\",\"gt\",\"nya\",\"kalo\",\"cb\",\"jg\",\"jgn\",\"gw\",\"ge\",\n","                 \"sy\",\"min\",\"mas\",\"mba\",\"mbak\",\"pak\",\"kak\",\"trus\",\"trs\",\"bs\",\"bisa\",\"aja\",\"saja\",\"no\",\n","                 \"w\",\"g\",\"gua\",\"gue\",\"emang\",\"emg\",\"wkwk\",\"dr\",\"kau\",\"dg\",\"gimana\",\"apapun\",\"apa\",\n","                 \"klo\",\"yah\",\"banget\",\"pake\",\"terus\",\"krn\",\"jadi\",\"jd\",\"mu\",\"ku\",\"si\",\"hehe\",\n","                 \"tp\",\"pa\",\"lu\",\"lo\",\"lw\",\"tw\",\"tau\",\"karna\",\"kayak\",\"ky\",\"lg\",\"untuk\",\"tuk\",\"dg\",\"dgn\"}\n","stopwords_all = stopwords_set.union(stopwords_aug)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4uDfBGqKxC4","colab_type":"code","colab":{}},"source":["#stopwords_set"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZSxXa_mxKxDF","colab_type":"code","colab":{}},"source":["def wordcloud_draw(data, color = 'black'):\n","    words = ' '.join(data)\n","    cleaned_word = \" \".join([word for word in words.split()\n","                            if 'http' not in word\n","                                and not word.startswith('@')\n","                                and not word.startswith('#')\n","                                and word != 'RT'\n","                            ])\n","    wordcloud = WordCloud(stopwords=stopwords_all,\n","                      background_color=color,\n","                      width=2500,\n","                      height=2000\n","                     ).generate(cleaned_word)\n","    plt.figure(1,figsize=(13, 13))\n","    plt.imshow(wordcloud)\n","    plt.axis('off')\n","    plt.show()\n","\n","print(\"Positive words\")\n","wordcloud_draw(train_pos,'white')\n","print(\"Negative words\")\n","wordcloud_draw(train_neg)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3uGeGkBKxDT","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"acEv2kePKxDj","colab_type":"code","colab":{}},"source":["tweets = []\n","#stopwords_set = set(stopwords.words(\"indonesia\"))\n","\n","for index, row in train.iterrows():\n","    words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3]\n","    words_cleaned = [word for word in words_filtered\n","        if 'http' not in word\n","        and not word.startswith('@')\n","        and not word.startswith('#')\n","        and word != 'RT']\n","    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_all]\n","    tweets.append((words_cleaned,row.sentiment))\n","\n","test_pos = test[ test['sentiment'] == 'Positif']\n","test_pos = test_pos['text']\n","test_neg = test[ test['sentiment'] == 'Negatif']\n","test_neg = test_neg['text']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLcJxOvAKxDw","colab_type":"code","colab":{}},"source":["test_pos"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xNT7CaoRKxD_","colab_type":"code","colab":{}},"source":["test_neg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mb-rQQpKKxEQ","colab_type":"code","colab":{}},"source":["# Extracting word features\n","def get_words_in_tweets(tweets):\n","    all = []\n","    for (words, sentiment) in tweets:\n","        all.extend(words)\n","    return all\n","\n","def get_word_features(wordlist):\n","    wordlist = nltk.FreqDist(wordlist)\n","    features = wordlist.keys()\n","    return features\n","w_features = get_word_features(get_words_in_tweets(tweets))\n","\n","\n","def extract_features(document):\n","    document_words = set(document)\n","    features = {}\n","    for word in w_features:\n","        features['containts(%s)' % word] = (word in document_words)\n","    return features\n","\n","\n","wordcloud_draw(w_features)\n","\n","# Training the Naive Bayes classifier\n","training_set = nltk.classify.apply_features(extract_features,tweets)\n","classifier = nltk.NaiveBayesClassifier.train(training_set)\n","\n","neg_cnt = 0\n","pos_cnt = 0\n","for obj in test_neg: \n","    res =  classifier.classify(extract_features(obj.split()))\n","    if(res == 'Negatif'): \n","        neg_cnt = neg_cnt + 1\n","for obj in test_pos: \n","    res =  classifier.classify(extract_features(obj.split()))\n","    if(res == 'Positif'): \n","        pos_cnt = pos_cnt + 1\n","        \n","print('[Negatif]: %s/%s '  % (len(test_neg),neg_cnt))        \n","print('[Positif]: %s/%s '  % (len(test_pos),pos_cnt))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUS6Jy9BKxEi","colab_type":"code","colab":{}},"source":["w_features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XVx4-9HJKxEy","colab_type":"code","colab":{}},"source":["nltk.FreqDist(get_words_in_tweets(tweets))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pZbVAuRrKxFB","colab_type":"code","colab":{}},"source":["tweets"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mU8pIFX4KxFZ","colab_type":"code","colab":{}},"source":["#To save the trained claassifier, do the following\n","import pickle\n","f = open('my_classifier.pickle', 'wb')\n","pickle.dump(classifier, f)\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E83M6QtPKxFp","colab_type":"code","colab":{}},"source":["#To reload it:\n","f = open('my_classifier.pickle', 'rb')\n","classifier = pickle.load(f)\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"137ybphvKxF4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}